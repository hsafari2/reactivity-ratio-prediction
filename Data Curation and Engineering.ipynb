{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b0222a-1532-44f0-b6b8-a1d83b3817bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: ELIMINATING NEGATIVE VALUES\n",
      "Initial dataset: 2991 entries\n",
      "Entries with negative values: 296\n",
      "Entries removed: 296 (9.90%)\n",
      "Remaining entries: 2695\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===============================================\n",
    "# STAGE 1: ELIMINATING NEGATIVE VALUES\n",
    "# ===============================================\n",
    "\n",
    "address = r'/work/bavarian/hsafari2/Manuscript Code/CoPolDB.xlsx'\n",
    "# ↑ Added the missing closing quote here\n",
    "\n",
    "# Load the original dataset\n",
    "df_original = pd.read_excel(address)\n",
    "\n",
    "print(\"STAGE 1: ELIMINATING NEGATIVE VALUES\")\n",
    "print(f\"Initial dataset: {len(df_original)} entries\")\n",
    "\n",
    "# Check for negative values\n",
    "negative_entries = len(df_original[(df_original['r1'] < 0) | (df_original['r2'] < 0)])\n",
    "print(f\"Entries with negative values: {negative_entries}\")\n",
    "\n",
    "# Remove entries with negative reactivity ratios\n",
    "df_stage1 = df_original[(df_original['r1'] >= 0) & (df_original['r2'] >= 0)].copy()\n",
    "\n",
    "# Report results\n",
    "entries_removed = len(df_original) - len(df_stage1)\n",
    "percentage_removed = (entries_removed / len(df_original)) * 100\n",
    "\n",
    "print(f\"Entries removed: {entries_removed} ({percentage_removed:.2f}%)\")\n",
    "print(f\"Remaining entries: {len(df_stage1)}\")\n",
    "\n",
    "# Optional: Save the filtered dataset\n",
    "#df_stage1.to_excel('CopolDB_Stage1_NoNegatives.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877972f7-d296-4cbc-a5c3-7b22df68f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2: REMOVING MULTI-VINYL MONOMERS\n",
      "Starting entries: 2695 entries\n",
      "Entries with multi-vinyl monomers: 330\n",
      "Entries removed: 330 (12.24%)\n",
      "Remaining entries: 2365\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STAGE 2: REMOVING MULTI-VINYL MONOMERS\n",
    "# ===============================================\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "def count_vinyl_groups(smiles):\n",
    "    \"\"\"Count non-aromatic vinyl (C=C) groups in a molecule\"\"\"\n",
    "    if pd.isna(smiles):\n",
    "        return 0\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return 0\n",
    "        \n",
    "        # Identify aromatic atoms\n",
    "        aromatic_atoms = set()\n",
    "        for atom in mol.GetAtoms():\n",
    "            if atom.GetIsAromatic():\n",
    "                aromatic_atoms.add(atom.GetIdx())\n",
    "        \n",
    "        # Count non-aromatic C=C double bonds\n",
    "        vinyl_count = 0\n",
    "        for bond in mol.GetBonds():\n",
    "            if (bond.GetBondType() == Chem.BondType.DOUBLE and \n",
    "                mol.GetAtomWithIdx(bond.GetBeginAtomIdx()).GetSymbol() == 'C' and\n",
    "                mol.GetAtomWithIdx(bond.GetEndAtomIdx()).GetSymbol() == 'C'):\n",
    "                \n",
    "                begin_idx = bond.GetBeginAtomIdx()\n",
    "                end_idx = bond.GetEndAtomIdx()\n",
    "                \n",
    "                # Only count if both carbons are non-aromatic\n",
    "                if begin_idx not in aromatic_atoms and end_idx not in aromatic_atoms:\n",
    "                    vinyl_count += 1\n",
    "        return vinyl_count\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "print(\"STAGE 2: REMOVING MULTI-VINYL MONOMERS\")\n",
    "print(f\"Starting entries: {len(df_stage1)} entries\")\n",
    "\n",
    "# Initialize counters\n",
    "multi_vinyl_entries = 0\n",
    "valid_rows = []\n",
    "\n",
    "# Process each row individually\n",
    "for index, row in df_stage1.iterrows():\n",
    "    vinyl_a = count_vinyl_groups(row['SMILES_A'])\n",
    "    vinyl_b = count_vinyl_groups(row['SMILES_B'])\n",
    "    \n",
    "    # Check if both monomers have exactly 1 vinyl group\n",
    "    if vinyl_a == 1 and vinyl_b == 1:\n",
    "        valid_rows.append(row)\n",
    "    else:\n",
    "        multi_vinyl_entries += 1\n",
    "\n",
    "print(f\"Entries with multi-vinyl monomers: {multi_vinyl_entries}\")\n",
    "\n",
    "# Create new dataframe from valid rows\n",
    "df_stage2 = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "\n",
    "# Report results\n",
    "entries_removed = len(df_stage1) - len(df_stage2)\n",
    "percentage_removed = (entries_removed / len(df_stage1)) * 100\n",
    "\n",
    "print(f\"Entries removed: {entries_removed} ({percentage_removed:.2f}%)\")\n",
    "print(f\"Remaining entries: {len(df_stage2)}\")\n",
    "\n",
    "# Optional: Save the filtered dataset\n",
    "#df_stage2.to_excel('CopolDB_Stage2_SingleVinyl.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f7955-7c1d-440b-9761-e2c7414671ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956dd2c-6329-44e6-b3e6-590f78435175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69221d18-0727-4453-ae99-6e114e6bbb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3: HANDLING REPEATED MONOMER PAIRS\n",
      "Starting entries: 2365 entries\n",
      "\n",
      "Step 1: Identifying unique monomer pairs...\n",
      "Unique monomer pairs identified: 1551\n",
      "\n",
      "Step 2: Analyzing measurement patterns...\n",
      "Pairs with single measurements: 1175\n",
      "Pairs with multiple measurements: 376\n",
      "\n",
      "Measurement count distribution:\n",
      "  1175 pairs have 1 measurement(s)\n",
      "  229 pairs have 2 measurement(s)\n",
      "  66 pairs have 3 measurement(s)\n",
      "  28 pairs have 4 measurement(s)\n",
      "  19 pairs have 5 measurement(s)\n",
      "  10 pairs have 6 measurement(s)\n",
      "  8 pairs have 7 measurement(s)\n",
      "  3 pairs have 8 measurement(s)\n",
      "  2 pairs have 9 measurement(s)\n",
      "  4 pairs have 10 measurement(s)\n",
      "  1 pairs have 12 measurement(s)\n",
      "  2 pairs have 13 measurement(s)\n",
      "  1 pairs have 14 measurement(s)\n",
      "  1 pairs have 17 measurement(s)\n",
      "  1 pairs have 20 measurement(s)\n",
      "  1 pairs have 40 measurement(s)\n",
      "\n",
      "Step 3: Applying normalized variance filter (threshold = 0.2)...\n",
      "\n",
      "Step 4: Final results summary...\n",
      "Single measurement pairs (kept): 1175\n",
      "Multiple measurement pairs processed: 376\n",
      "Multiple measurement pairs that passed variance test: 169\n",
      "Multiple measurement pairs excluded (high variance): 207\n",
      "\n",
      "Overall Stage 3 impact:\n",
      "Total entries removed: 1021 (43.17%)\n",
      "Reliable unique pairs retained: 1344\n",
      "Percentage of multiple-measurement pairs excluded: 55.1%\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STAGE 3: HANDLING REPEATED MONOMER PAIRS\n",
    "# ===============================================\n",
    "\n",
    "def calculate_normalized_variance(values):\n",
    "    \"\"\"\n",
    "    Calculate normalized variance (σ²/μ²)\n",
    "    \n",
    "    This gives us a dimensionless measure of how much the experimental \n",
    "    values vary relative to their average. For example:\n",
    "    - If mean = 1.0 and variance = 0.04, normalized variance = 0.04/1.0² = 0.04\n",
    "    - If mean = 10.0 and variance = 4.0, normalized variance = 4.0/10.0² = 0.04\n",
    "    Both have the same relative variability despite different absolute scales.\n",
    "    \"\"\"\n",
    "    if len(values) <= 1:\n",
    "        return 0.0  # Can't calculate variance with single value\n",
    "    \n",
    "    mean_val = np.mean(values)\n",
    "    if mean_val == 0:\n",
    "        return float('inf')  # Avoid division by zero\n",
    "    \n",
    "    # Use sample variance (ddof=1) for unbiased estimation\n",
    "    variance = np.var(values, ddof=1)\n",
    "    normalized_variance = variance / (mean_val ** 2)\n",
    "    \n",
    "    return normalized_variance\n",
    "\n",
    "print(\"STAGE 3: HANDLING REPEATED MONOMER PAIRS\")\n",
    "print(f\"Starting entries: {len(df_stage2)} entries\")\n",
    "\n",
    "# Step 1: Group entries by exact monomer pair identity\n",
    "print(\"\\nStep 1: Identifying unique monomer pairs...\")\n",
    "\n",
    "pair_groups = {}\n",
    "for index, row in df_stage2.iterrows():\n",
    "    # Create unique identifier: each (MonomerA, MonomerB) combination\n",
    "    pair_key = (row['MonomerA'], row['MonomerB'])\n",
    "    \n",
    "    if pair_key not in pair_groups:\n",
    "        pair_groups[pair_key] = []\n",
    "    \n",
    "    # Store complete row information for this pair\n",
    "    pair_groups[pair_key].append({\n",
    "        'MonomerA': row['MonomerA'],\n",
    "        'MonomerB': row['MonomerB'], \n",
    "        'SMILES_A': row['SMILES_A'],\n",
    "        'SMILES_B': row['SMILES_B'],\n",
    "        'r1': row['r1'],\n",
    "        'r2': row['r2']\n",
    "    })\n",
    "\n",
    "total_unique_pairs = len(pair_groups)\n",
    "print(f\"Unique monomer pairs identified: {total_unique_pairs}\")\n",
    "\n",
    "# Step 2: Analyze measurement distribution patterns\n",
    "print(\"\\nStep 2: Analyzing measurement patterns...\")\n",
    "\n",
    "single_measurement_pairs = 0\n",
    "multiple_measurement_pairs = 0\n",
    "measurement_distribution = {}\n",
    "\n",
    "for pair_key, measurements in pair_groups.items():\n",
    "    num_measurements = len(measurements)\n",
    "    \n",
    "    # Track how many measurements each pair has\n",
    "    if num_measurements not in measurement_distribution:\n",
    "        measurement_distribution[num_measurements] = 0\n",
    "    measurement_distribution[num_measurements] += 1\n",
    "    \n",
    "    # Categorize pairs\n",
    "    if num_measurements == 1:\n",
    "        single_measurement_pairs += 1\n",
    "    else:\n",
    "        multiple_measurement_pairs += 1\n",
    "\n",
    "print(f\"Pairs with single measurements: {single_measurement_pairs}\")\n",
    "print(f\"Pairs with multiple measurements: {multiple_measurement_pairs}\")\n",
    "\n",
    "# Show distribution of measurement counts\n",
    "print(f\"\\nMeasurement count distribution:\")\n",
    "for count in sorted(measurement_distribution.keys()):\n",
    "    print(f\"  {measurement_distribution[count]} pairs have {count} measurement(s)\")\n",
    "\n",
    "# Step 3: Process pairs with multiple measurements using variance analysis\n",
    "print(f\"\\nStep 3: Applying normalized variance filter (threshold = 0.2)...\")\n",
    "\n",
    "variance_threshold = 0.2\n",
    "valid_single_pairs = []        # Pairs with one measurement (automatically valid)\n",
    "valid_averaged_pairs = []      # Pairs with multiple measurements that pass variance test  \n",
    "excluded_high_variance = []    # Pairs excluded due to high variance\n",
    "\n",
    "# Detailed analysis for manuscript reporting\n",
    "total_multiple_measurements_processed = 0\n",
    "\n",
    "for pair_key, measurements in pair_groups.items():\n",
    "    \n",
    "    if len(measurements) == 1:\n",
    "        # Single measurement: no variance to calculate, automatically include\n",
    "        valid_single_pairs.append(measurements[0])\n",
    "        \n",
    "    else:\n",
    "        # Multiple measurements: apply variance analysis\n",
    "        total_multiple_measurements_processed += 1\n",
    "        \n",
    "        # Extract r1 and r2 values from all measurements of this pair\n",
    "        r1_values = [m['r1'] for m in measurements]\n",
    "        r2_values = [m['r2'] for m in measurements]\n",
    "        \n",
    "        # Calculate normalized variance for both reactivity ratios\n",
    "        r1_norm_variance = calculate_normalized_variance(r1_values)\n",
    "        r2_norm_variance = calculate_normalized_variance(r2_values)\n",
    "        \n",
    "        # Apply dual threshold: BOTH r1 and r2 must have low variance\n",
    "        if r1_norm_variance < variance_threshold and r2_norm_variance < variance_threshold:\n",
    "            # Passed variance test: average the values\n",
    "            averaged_entry = measurements[0].copy()  # Use first entry as template\n",
    "            averaged_entry['r1'] = np.mean(r1_values)\n",
    "            averaged_entry['r2'] = np.mean(r2_values)\n",
    "            valid_averaged_pairs.append(averaged_entry)\n",
    "            \n",
    "        else:\n",
    "            # Failed variance test: exclude as unreliable\n",
    "            excluded_high_variance.append({\n",
    "                'pair': pair_key,\n",
    "                'measurements': len(measurements),\n",
    "                'r1_variance': r1_norm_variance,\n",
    "                'r2_variance': r2_norm_variance\n",
    "            })\n",
    "\n",
    "# Step 4: Combine all valid pairs into final dataset\n",
    "all_reliable_pairs = valid_single_pairs + valid_averaged_pairs\n",
    "df_stage3 = pd.DataFrame(all_reliable_pairs).reset_index(drop=True)\n",
    "\n",
    "# Comprehensive reporting for manuscript\n",
    "print(f\"\\nStep 4: Final results summary...\")\n",
    "print(f\"Single measurement pairs (kept): {len(valid_single_pairs)}\")\n",
    "print(f\"Multiple measurement pairs processed: {total_multiple_measurements_processed}\")\n",
    "print(f\"Multiple measurement pairs that passed variance test: {len(valid_averaged_pairs)}\")\n",
    "print(f\"Multiple measurement pairs excluded (high variance): {len(excluded_high_variance)}\")\n",
    "\n",
    "# Calculate removal statistics\n",
    "total_entries_removed = len(df_stage2) - len(df_stage3)\n",
    "percentage_removed = (total_entries_removed / len(df_stage2)) * 100\n",
    "pairs_excluded_percentage = (len(excluded_high_variance) / total_multiple_measurements_processed) * 100 if total_multiple_measurements_processed > 0 else 0\n",
    "\n",
    "print(f\"\\nOverall Stage 3 impact:\")\n",
    "print(f\"Total entries removed: {total_entries_removed} ({percentage_removed:.2f}%)\")\n",
    "print(f\"Reliable unique pairs retained: {len(df_stage3)}\")\n",
    "print(f\"Percentage of multiple-measurement pairs excluded: {pairs_excluded_percentage:.1f}%\")\n",
    "\n",
    "# Optional: Save the reliable unique pairs dataset\n",
    "#df_stage3.to_excel('CopolDB_Stage3_ReliableUnique.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4ea4ce-9f00-4145-a4ea-bddd53a5edf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 4: DATA AUGMENTATION USING SYMMETRY\n",
      "Starting with: 1344 reliable unique pairs\n",
      "\n",
      "DataFrame columns: ['MonomerA', 'MonomerB', 'SMILES_A', 'SMILES_B', 'r1', 'r2']\n",
      "\n",
      "Processing each monomer pair for symmetry augmentation...\n",
      "Format: Original -> Swapped\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Row   1:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Vinylidene chloride      \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: C=C(Cl)Cl                     \n",
      "    r1: 3.3680 | r2: 0.1540\n",
      "  Swapped:\n",
      "    MonomerA: Vinylidene chloride       | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: C=C(Cl)Cl                      | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.1540 | r2: 3.3680\n",
      "  ================================================================================\n",
      "\n",
      "Row   2:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: 2,3-Dichloro-1-propene   \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: C=C(CCl)Cl                    \n",
      "    r1: 4.0000 | r2: 0.0000\n",
      "  Swapped:\n",
      "    MonomerA: 2,3-Dichloro-1-propene    | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: C=C(CCl)Cl                     | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.0000 | r2: 4.0000\n",
      "  ================================================================================\n",
      "\n",
      "Row   3:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Ethyl methacrylate       \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CCOC(=O)C(=C)C                \n",
      "    r1: 0.5700 | r2: 0.7100\n",
      "  Swapped:\n",
      "    MonomerA: Ethyl methacrylate        | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CCOC(=O)C(=C)C                 | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.7100 | r2: 0.5700\n",
      "  ================================================================================\n",
      "\n",
      "Row   4:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Isobutyl methacrylate    \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CC(C)COC(=O)C(=C)C            \n",
      "    r1: 2.0100 | r2: 0.4700\n",
      "  Swapped:\n",
      "    MonomerA: Isobutyl methacrylate     | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CC(C)COC(=O)C(=C)C             | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.4700 | r2: 2.0100\n",
      "  ================================================================================\n",
      "\n",
      "Row   5:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Glycidyl methacrylate    \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CC(=C)C(=O)OCC1CO1            \n",
      "    r1: 0.9800 | r2: 1.2000\n",
      "  Swapped:\n",
      "    MonomerA: Glycidyl methacrylate     | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CC(=C)C(=O)OCC1CO1             | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 1.2000 | r2: 0.9800\n",
      "  ================================================================================\n",
      "\n",
      "Row   6:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Vinyl acetate            \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CC(=O)OC=C                    \n",
      "    r1: 0.2000 | r2: 0.0100\n",
      "  Swapped:\n",
      "    MonomerA: Vinyl acetate             | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CC(=O)OC=C                     | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.0100 | r2: 0.2000\n",
      "  ================================================================================\n",
      "\n",
      "Row   7:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: 2-Methyl-5-vinylpyridine \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CC1=NC=C(C=C1)C=C             \n",
      "    r1: 0.4300 | r2: 0.8500\n",
      "  Swapped:\n",
      "    MonomerA: 2-Methyl-5-vinylpyridine  | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CC1=NC=C(C=C1)C=C              | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.8500 | r2: 0.4300\n",
      "  ================================================================================\n",
      "\n",
      "Row   8:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Butyl acrylate           \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: CCCCOC(=O)C=C                 \n",
      "    r1: 1.2500 | r2: 0.3100\n",
      "  Swapped:\n",
      "    MonomerA: Butyl acrylate            | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: CCCCOC(=O)C=C                  | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.3100 | r2: 1.2500\n",
      "  ================================================================================\n",
      "\n",
      "Row   9:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: Diethylene glycol monovinyl ether\n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: C=COCCOCCO                    \n",
      "    r1: 0.0000 | r2: 2.2000\n",
      "  Swapped:\n",
      "    MonomerA: Diethylene glycol monovinyl ether | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: C=COCCOCCO                     | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 2.2000 | r2: 0.0000\n",
      "  ================================================================================\n",
      "\n",
      "Row  10:\n",
      "  Original:\n",
      "    MonomerA: Methacrylic acid          | MonomerB: 2-Chlorostyrene          \n",
      "    SMILES_A: CC(=C)C(=O)O                   | SMILES_B: C=CC1=CC=CC=C1Cl              \n",
      "    r1: 0.6220 | r2: 0.1010\n",
      "  Swapped:\n",
      "    MonomerA: 2-Chlorostyrene           | MonomerB: Methacrylic acid         \n",
      "    SMILES_A: C=CC1=CC=CC=C1Cl               | SMILES_B: CC(=C)C(=O)O                  \n",
      "    r1: 0.1010 | r2: 0.6220\n",
      "  ================================================================================\n",
      "\n",
      "  ... (showing first 10 transformations, continuing processing)\n",
      "\n",
      "\n",
      "Step 4: Augmentation Results Summary\n",
      "Original reliable pairs: 1344\n",
      "After symmetry augmentation: 2688\n",
      "Augmentation factor: 2.0x\n",
      "New entries added: 1344\n",
      "\n",
      "Step 5: Verification checks...\n",
      "✓ Dataset size verification: 2688 = 2 × 1344\n",
      "✓ Entry type distribution:\n",
      "  original: 1344 entries\n",
      "  swapped: 1344 entries\n",
      "✓ Data integrity check: 0 missing values (should be 0)\n",
      "\n",
      "Step 6: Detailed symmetry verification (first 3 pairs):\n",
      "\n",
      "Pair 1 Verification:\n",
      "  Original entry:\n",
      "    MonomerA: 'Methacrylic acid' | MonomerB: 'Vinylidene chloride'\n",
      "    SMILES_A: 'CC(=C)C(=O)O' | SMILES_B: 'C=C(Cl)Cl'\n",
      "    r1: 3.3680 | r2: 0.1540\n",
      "  Swapped entry:\n",
      "    MonomerA: 'Vinylidene chloride' | MonomerB: 'Methacrylic acid'\n",
      "    SMILES_A: 'C=C(Cl)Cl' | SMILES_B: 'CC(=C)C(=O)O'\n",
      "    r1: 0.1540 | r2: 3.3680\n",
      "  Verification results:\n",
      "    Names swapped correctly: True\n",
      "    SMILES swapped correctly: True\n",
      "    Reactivity ratios swapped correctly: True\n",
      "    Overall symmetry verified: True\n",
      "\n",
      "Pair 2 Verification:\n",
      "  Original entry:\n",
      "    MonomerA: 'Methacrylic acid' | MonomerB: '2,3-Dichloro-1-propene'\n",
      "    SMILES_A: 'CC(=C)C(=O)O' | SMILES_B: 'C=C(CCl)Cl'\n",
      "    r1: 4.0000 | r2: 0.0000\n",
      "  Swapped entry:\n",
      "    MonomerA: '2,3-Dichloro-1-propene' | MonomerB: 'Methacrylic acid'\n",
      "    SMILES_A: 'C=C(CCl)Cl' | SMILES_B: 'CC(=C)C(=O)O'\n",
      "    r1: 0.0000 | r2: 4.0000\n",
      "  Verification results:\n",
      "    Names swapped correctly: True\n",
      "    SMILES swapped correctly: True\n",
      "    Reactivity ratios swapped correctly: True\n",
      "    Overall symmetry verified: True\n",
      "\n",
      "Pair 3 Verification:\n",
      "  Original entry:\n",
      "    MonomerA: 'Methacrylic acid' | MonomerB: 'Ethyl methacrylate'\n",
      "    SMILES_A: 'CC(=C)C(=O)O' | SMILES_B: 'CCOC(=O)C(=C)C'\n",
      "    r1: 0.5700 | r2: 0.7100\n",
      "  Swapped entry:\n",
      "    MonomerA: 'Ethyl methacrylate' | MonomerB: 'Methacrylic acid'\n",
      "    SMILES_A: 'CCOC(=O)C(=C)C' | SMILES_B: 'CC(=C)C(=O)O'\n",
      "    r1: 0.7100 | r2: 0.5700\n",
      "  Verification results:\n",
      "    Names swapped correctly: True\n",
      "    SMILES swapped correctly: True\n",
      "    Reactivity ratios swapped correctly: True\n",
      "    Overall symmetry verified: True\n",
      "\n",
      "================================================================================\n",
      "STAGE 4 COMPLETE!\n",
      "================================================================================\n",
      "Final augmented dataset: 2688 entries\n",
      "All 6 columns properly swapped:\n",
      "  - MonomerA ↔ MonomerB\n",
      "  - SMILES_A ↔ SMILES_B\n",
      "  - r1 ↔ r2\n",
      "Ready for feature extraction and modeling stages.\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STAGE 4: DATA AUGMENTATION USING SYMMETRY\n",
    "# ===============================================\n",
    "\n",
    "print(\"STAGE 4: DATA AUGMENTATION USING SYMMETRY\")\n",
    "print(f\"Starting with: {len(df_stage3)} reliable unique pairs\")\n",
    "\n",
    "# Check the actual columns in the dataframe\n",
    "print(f\"\\nDataFrame columns: {list(df_stage3.columns)}\")\n",
    "\n",
    "# Initialize lists to store original and swapped data\n",
    "augmented_data = []\n",
    "\n",
    "print(\"\\nProcessing each monomer pair for symmetry augmentation...\")\n",
    "print(\"Format: Original -> Swapped\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Process each row for augmentation\n",
    "for index, row in df_stage3.iterrows():\n",
    "    \n",
    "    # Step 1: Add the original entry as-is\n",
    "    original_entry = {\n",
    "        'MonomerA': row['MonomerA'],\n",
    "        'MonomerB': row['MonomerB'],\n",
    "        'SMILES_A': row['SMILES_A'],\n",
    "        'SMILES_B': row['SMILES_B'],\n",
    "        'r1': row['r1'],\n",
    "        'r2': row['r2'],\n",
    "        'entry_type': 'original'  # Track entry type for verification\n",
    "    }\n",
    "    augmented_data.append(original_entry)\n",
    "    \n",
    "    # Step 2: Create the swapped entry leveraging copolymerization symmetry\n",
    "    # In copolymerization: if (A,B) has reactivity ratios (r1, r2)\n",
    "    # then (B,A) has reactivity ratios (r2, r1) due to symmetry\n",
    "    swapped_entry = {\n",
    "        'MonomerA': row['MonomerB'],        # Swap: MonomerA ↔ MonomerB\n",
    "        'MonomerB': row['MonomerA'],        # Swap: MonomerB ↔ MonomerA\n",
    "        'SMILES_A': row['SMILES_B'],        # Swap: SMILES_A ↔ SMILES_B\n",
    "        'SMILES_B': row['SMILES_A'],        # Swap: SMILES_B ↔ SMILES_A\n",
    "        'r1': row['r2'],                    # Swap: r1 ↔ r2\n",
    "        'r2': row['r1'],                    # Swap: r2 ↔ r1\n",
    "        'entry_type': 'swapped'             # Track entry type for verification\n",
    "    }\n",
    "    augmented_data.append(swapped_entry)\n",
    "    \n",
    "    # Display the transformation for verification (show first 10 for brevity)\n",
    "    if index < 10:\n",
    "        print(f\"Row {index+1:3d}:\")\n",
    "        print(f\"  Original:\")\n",
    "        print(f\"    MonomerA: {row['MonomerA']:25s} | MonomerB: {row['MonomerB']:25s}\")\n",
    "        print(f\"    SMILES_A: {row['SMILES_A']:30s} | SMILES_B: {row['SMILES_B']:30s}\")\n",
    "        print(f\"    r1: {row['r1']:.4f} | r2: {row['r2']:.4f}\")\n",
    "        print(f\"  Swapped:\")\n",
    "        print(f\"    MonomerA: {row['MonomerB']:25s} | MonomerB: {row['MonomerA']:25s}\")\n",
    "        print(f\"    SMILES_A: {row['SMILES_B']:30s} | SMILES_B: {row['SMILES_A']:30s}\")\n",
    "        print(f\"    r1: {row['r2']:.4f} | r2: {row['r1']:.4f}\")\n",
    "        print(f\"  {'='*80}\")\n",
    "        print()\n",
    "    elif index == 10:\n",
    "        print(\"  ... (showing first 10 transformations, continuing processing)\")\n",
    "        print()\n",
    "\n",
    "# Step 3: Create the final augmented dataframe\n",
    "df_stage4 = pd.DataFrame(augmented_data).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Comprehensive reporting\n",
    "original_count = len(df_stage3)\n",
    "augmented_count = len(df_stage4)\n",
    "augmentation_factor = augmented_count / original_count\n",
    "\n",
    "print(f\"\\nStep 4: Augmentation Results Summary\")\n",
    "print(f\"Original reliable pairs: {original_count}\")\n",
    "print(f\"After symmetry augmentation: {augmented_count}\")\n",
    "print(f\"Augmentation factor: {augmentation_factor:.1f}x\")\n",
    "print(f\"New entries added: {augmented_count - original_count}\")\n",
    "\n",
    "# Step 5: Verification of augmentation quality\n",
    "print(f\"\\nStep 5: Verification checks...\")\n",
    "\n",
    "# Check 1: Verify we have exactly double the data\n",
    "assert augmented_count == 2 * original_count, \"Augmentation should exactly double the dataset!\"\n",
    "print(f\"✓ Dataset size verification: {augmented_count} = 2 × {original_count}\")\n",
    "\n",
    "# Check 2: Verify entry type distribution\n",
    "entry_type_counts = df_stage4['entry_type'].value_counts()\n",
    "print(f\"✓ Entry type distribution:\")\n",
    "for entry_type, count in entry_type_counts.items():\n",
    "    print(f\"  {entry_type}: {count} entries\")\n",
    "\n",
    "# Check 3: Verify no data loss in critical columns\n",
    "required_columns = ['MonomerA', 'MonomerB', 'SMILES_A', 'SMILES_B', 'r1', 'r2']\n",
    "missing_data = df_stage4[required_columns].isnull().sum().sum()\n",
    "print(f\"✓ Data integrity check: {missing_data} missing values (should be 0)\")\n",
    "\n",
    "# Check 4: Detailed verification of ALL 6 columns for first 3 pairs\n",
    "print(f\"\\nStep 6: Detailed symmetry verification (first 3 pairs):\")\n",
    "for i in range(0, min(6, len(df_stage4)), 2):  # Check first 3 original-swapped pairs\n",
    "    orig = df_stage4.iloc[i]\n",
    "    swap = df_stage4.iloc[i+1]\n",
    "    \n",
    "    print(f\"\\nPair {i//2 + 1} Verification:\")\n",
    "    print(f\"  Original entry:\")\n",
    "    print(f\"    MonomerA: '{orig['MonomerA']}' | MonomerB: '{orig['MonomerB']}'\")\n",
    "    print(f\"    SMILES_A: '{orig['SMILES_A']}' | SMILES_B: '{orig['SMILES_B']}'\")\n",
    "    print(f\"    r1: {orig['r1']:.4f} | r2: {orig['r2']:.4f}\")\n",
    "    \n",
    "    print(f\"  Swapped entry:\")\n",
    "    print(f\"    MonomerA: '{swap['MonomerA']}' | MonomerB: '{swap['MonomerB']}'\")\n",
    "    print(f\"    SMILES_A: '{swap['SMILES_A']}' | SMILES_B: '{swap['SMILES_B']}'\")\n",
    "    print(f\"    r1: {swap['r1']:.4f} | r2: {swap['r2']:.4f}\")\n",
    "    \n",
    "    # Verify ALL symmetry relationships\n",
    "    name_swap_check = (orig['MonomerA'] == swap['MonomerB'] and orig['MonomerB'] == swap['MonomerA'])\n",
    "    smiles_swap_check = (orig['SMILES_A'] == swap['SMILES_B'] and orig['SMILES_B'] == swap['SMILES_A'])\n",
    "    r_swap_check = (abs(orig['r1'] - swap['r2']) < 1e-10 and abs(orig['r2'] - swap['r1']) < 1e-10)\n",
    "    \n",
    "    print(f\"  Verification results:\")\n",
    "    print(f\"    Names swapped correctly: {name_swap_check}\")\n",
    "    print(f\"    SMILES swapped correctly: {smiles_swap_check}\")\n",
    "    print(f\"    Reactivity ratios swapped correctly: {r_swap_check}\")\n",
    "    print(f\"    Overall symmetry verified: {name_swap_check and smiles_swap_check and r_swap_check}\")\n",
    "\n",
    "# Optional: Remove the entry_type column for final dataset (if not needed for analysis)\n",
    "df_stage4_final = df_stage4.drop('entry_type', axis=1)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 4 COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Final augmented dataset: {len(df_stage4_final)} entries\")\n",
    "print(f\"All 6 columns properly swapped:\")\n",
    "print(f\"  - MonomerA ↔ MonomerB\")\n",
    "print(f\"  - SMILES_A ↔ SMILES_B\") \n",
    "print(f\"  - r1 ↔ r2\")\n",
    "print(f\"Ready for feature extraction and modeling stages.\")\n",
    "\n",
    "# Optional: Save the augmented dataset\n",
    "#df_stage4_final.to_excel('CopolDB_Stage4_Augmented.xlsx', index=False)\n",
    "#print(f\"Augmented dataset saved to: CopolDB_Stage4_Augmented.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1863e2f5-c43b-4a93-83cc-5f07bb3d378b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 5: REACTIVITY RATIO RANGE FILTERING\n",
      "Starting with: 2688 augmented entries\n",
      "\n",
      "Filtering criteria:\n",
      "  Minimum reactivity ratio threshold: 0.01\n",
      "  Maximum reactivity ratio threshold: 10.0\n",
      "  Both r1 and r2 must be within [0.01, 10.0]\n",
      "\n",
      "Step 1: Analyzing current reactivity ratio distribution...\n",
      "r1 statistics:\n",
      "  Min: 0.000000, Max: 830.000000\n",
      "  Mean: 2.1310, Median: 0.4700\n",
      "r2 statistics:\n",
      "  Min: 0.000000, Max: 830.000000\n",
      "  Mean: 2.1310, Median: 0.4700\n",
      "\n",
      "Step 2: Identifying entries outside acceptable range...\n",
      "Problematic entries analysis:\n",
      "  r1 < 0.01: 124 entries\n",
      "  r1 > 10.0: 92 entries\n",
      "  r2 < 0.01: 124 entries\n",
      "  r2 > 10.0: 92 entries\n",
      "  Total entries with extreme values: 384\n",
      "\n",
      "Step 3: Applying range filtering...\n",
      "\n",
      "Step 4: Filtering results summary...\n",
      "Entries before filtering: 2688\n",
      "Entries after filtering: 2304\n",
      "Entries removed: 384 (14.29%)\n",
      "\n",
      "Step 5: Verification for logarithmic transformation...\n",
      "Final reactivity ratio ranges:\n",
      "  r1: [0.0100, 10.0000]\n",
      "  r2: [0.0100, 10.0000]\n",
      "✓ Logarithmic transformation feasible\n",
      "  log(r1) range: [-4.6052, 2.3026]\n",
      "  log(r2) range: [-4.6052, 2.3026]\n",
      "\n",
      "Step 6: Final dataset characteristics...\n",
      "Final dataset size: 2304 entries\n",
      "Reactivity ratio distribution after filtering:\n",
      "  r1 - Mean: 0.9498, Std: 1.3735\n",
      "  r2 - Mean: 0.9498, Std: 1.3735\n",
      "\n",
      "================================================================================\n",
      "STAGE 5 COMPLETE!\n",
      "================================================================================\n",
      "Final curated dataset ready for feature extraction:\n",
      "  - 2304 high-quality entries\n",
      "  - Reactivity ratios in practical range [0.01, 10.0]\n",
      "  - Compatible with logarithmic transformation\n",
      "  - Suitable for machine learning model training\n",
      "Final dataset saved to: CopolDB_Stage5_Final.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# STAGE 5: REACTIVITY RATIO RANGE FILTERING\n",
    "# ===============================================\n",
    "\n",
    "print(\"STAGE 5: REACTIVITY RATIO RANGE FILTERING\")\n",
    "print(f\"Starting with: {len(df_stage4_final)} augmented entries\")\n",
    "\n",
    "# Define filtering bounds\n",
    "min_threshold = 0.01\n",
    "max_threshold = 10.0\n",
    "\n",
    "print(f\"\\nFiltering criteria:\")\n",
    "print(f\"  Minimum reactivity ratio threshold: {min_threshold}\")\n",
    "print(f\"  Maximum reactivity ratio threshold: {max_threshold}\")\n",
    "print(f\"  Both r1 and r2 must be within [{min_threshold}, {max_threshold}]\")\n",
    "\n",
    "# Step 1: Analyze current data distribution\n",
    "print(f\"\\nStep 1: Analyzing current reactivity ratio distribution...\")\n",
    "\n",
    "r1_stats = df_stage4_final['r1'].describe()\n",
    "r2_stats = df_stage4_final['r2'].describe()\n",
    "\n",
    "print(f\"r1 statistics:\")\n",
    "print(f\"  Min: {r1_stats['min']:.6f}, Max: {r1_stats['max']:.6f}\")\n",
    "print(f\"  Mean: {r1_stats['mean']:.4f}, Median: {r1_stats['50%']:.4f}\")\n",
    "\n",
    "print(f\"r2 statistics:\")\n",
    "print(f\"  Min: {r2_stats['min']:.6f}, Max: {r2_stats['max']:.6f}\")\n",
    "print(f\"  Mean: {r2_stats['mean']:.4f}, Median: {r2_stats['50%']:.4f}\")\n",
    "\n",
    "# Step 2: Identify problematic entries\n",
    "print(f\"\\nStep 2: Identifying entries outside acceptable range...\")\n",
    "\n",
    "# Find entries with extreme values\n",
    "r1_too_low = (df_stage4_final['r1'] < min_threshold).sum()\n",
    "r1_too_high = (df_stage4_final['r1'] > max_threshold).sum()\n",
    "r2_too_low = (df_stage4_final['r2'] < min_threshold).sum()\n",
    "r2_too_high = (df_stage4_final['r2'] > max_threshold).sum()\n",
    "\n",
    "print(f\"Problematic entries analysis:\")\n",
    "print(f\"  r1 < {min_threshold}: {r1_too_low} entries\")\n",
    "print(f\"  r1 > {max_threshold}: {r1_too_high} entries\")\n",
    "print(f\"  r2 < {min_threshold}: {r2_too_low} entries\")\n",
    "print(f\"  r2 > {max_threshold}: {r2_too_high} entries\")\n",
    "\n",
    "# Find entries where EITHER r1 OR r2 is outside range\n",
    "extreme_mask = ((df_stage4_final['r1'] < min_threshold) | \n",
    "                (df_stage4_final['r1'] > max_threshold) |\n",
    "                (df_stage4_final['r2'] < min_threshold) | \n",
    "                (df_stage4_final['r2'] > max_threshold))\n",
    "\n",
    "extreme_entries = extreme_mask.sum()\n",
    "print(f\"  Total entries with extreme values: {extreme_entries}\")\n",
    "\n",
    "# Step 3: Apply filtering\n",
    "print(f\"\\nStep 3: Applying range filtering...\")\n",
    "\n",
    "# Keep entries where BOTH r1 AND r2 are within acceptable range\n",
    "valid_mask = ((df_stage4_final['r1'] >= min_threshold) & \n",
    "              (df_stage4_final['r1'] <= max_threshold) &\n",
    "              (df_stage4_final['r2'] >= min_threshold) & \n",
    "              (df_stage4_final['r2'] <= max_threshold))\n",
    "\n",
    "df_stage5 = df_stage4_final[valid_mask].reset_index(drop=True)\n",
    "\n",
    "# Step 4: Report filtering results\n",
    "entries_before = len(df_stage4_final)\n",
    "entries_after = len(df_stage5)\n",
    "entries_removed = entries_before - entries_after\n",
    "removal_percentage = (entries_removed / entries_before) * 100\n",
    "\n",
    "print(f\"\\nStep 4: Filtering results summary...\")\n",
    "print(f\"Entries before filtering: {entries_before}\")\n",
    "print(f\"Entries after filtering: {entries_after}\")\n",
    "print(f\"Entries removed: {entries_removed} ({removal_percentage:.2f}%)\")\n",
    "\n",
    "# Step 5: Verify logarithmic transformation feasibility\n",
    "print(f\"\\nStep 5: Verification for logarithmic transformation...\")\n",
    "\n",
    "# Check if all values are positive (required for log transformation)\n",
    "min_r1 = df_stage5['r1'].min()\n",
    "min_r2 = df_stage5['r2'].min()\n",
    "max_r1 = df_stage5['r1'].max()\n",
    "max_r2 = df_stage5['r2'].max()\n",
    "\n",
    "print(f\"Final reactivity ratio ranges:\")\n",
    "print(f\"  r1: [{min_r1:.4f}, {max_r1:.4f}]\")\n",
    "print(f\"  r2: [{min_r2:.4f}, {max_r2:.4f}]\")\n",
    "\n",
    "# Test logarithmic transformation\n",
    "try:\n",
    "    import numpy as np\n",
    "    log_r1_test = np.log(df_stage5['r1'])\n",
    "    log_r2_test = np.log(df_stage5['r2'])\n",
    "    print(f\"✓ Logarithmic transformation feasible\")\n",
    "    print(f\"  log(r1) range: [{log_r1_test.min():.4f}, {log_r1_test.max():.4f}]\")\n",
    "    print(f\"  log(r2) range: [{log_r2_test.min():.4f}, {log_r2_test.max():.4f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Logarithmic transformation failed: {e}\")\n",
    "\n",
    "# Step 6: Final dataset statistics\n",
    "print(f\"\\nStep 6: Final dataset characteristics...\")\n",
    "print(f\"Final dataset size: {len(df_stage5)} entries\")\n",
    "print(f\"Reactivity ratio distribution after filtering:\")\n",
    "\n",
    "final_r1_stats = df_stage5['r1'].describe()\n",
    "final_r2_stats = df_stage5['r2'].describe()\n",
    "\n",
    "print(f\"  r1 - Mean: {final_r1_stats['mean']:.4f}, Std: {final_r1_stats['std']:.4f}\")\n",
    "print(f\"  r2 - Mean: {final_r2_stats['mean']:.4f}, Std: {final_r2_stats['std']:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 5 COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Final curated dataset ready for feature extraction:\")\n",
    "print(f\"  - {len(df_stage5)} high-quality entries\")\n",
    "print(f\"  - Reactivity ratios in practical range [{min_threshold}, {max_threshold}]\")\n",
    "print(f\"  - Compatible with logarithmic transformation\")\n",
    "print(f\"  - Suitable for machine learning model training\")\n",
    "\n",
    "# Optional: Save the final curated dataset\n",
    "df_stage5.to_excel('Dataset.xlsx', index=False)\n",
    "print(f\"Final dataset saved to: CopolDB_Stage5_Final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59be78b-6217-4f20-8e9e-f63e39b664f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HabibKernel",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
